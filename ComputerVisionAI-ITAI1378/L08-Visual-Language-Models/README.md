# L08 – Visual Language Models (VLMs)

## Overview
This lab introduced Visual Language Models (VLMs), which combine computer vision with natural language processing. These models, such as BLIP or CLIP, can generate captions, match text to images, or perform zero-shot classification.

## Concepts Covered
- Vision-language
- Classification, image search, and caption 
- Evaluate outputs using appropriate metrics
- Learn about fine-tuning and trade-off options

## Skills Learned
- Describe how VLMs connect vision and language
- Implemented CLIP for classification, image search, caption, and visual question answering  
- I learned about the ethical considerations built into AI systems

## Files
- `L08_VLMs.ipynb` — experiments and outputs 
